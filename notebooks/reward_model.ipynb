{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, AutoTokenizer\n",
    ")\n",
    "\n",
    "from peft import get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "from trl import (\n",
    "    ModelConfig,\n",
    "    RewardConfig,\n",
    "    RewardTrainer,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "\n",
    "from fed_ppo.callbacks import WeightChangeCallback\n",
    "from fed_ppo.utils import (\n",
    "    custom_optimizer, \n",
    "    apply_chat_template,\n",
    "    tokenize,\n",
    "    OptimizerConfig,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visible devices\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "VISIBLE_DEVICES = \"2\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Enumerate GPUs based on their PCI bus IDs\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{VISIBLE_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "MODEL_PATH = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "MODEL_NAME = MODEL_PATH.split('/')[1]\n",
    "\n",
    "# Dataset path\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "DATASET_PATH        = \"trl-lib/ultrafeedback_binarized\"\n",
    "DATASET_TRAIN_SPLIT = \"train\"\n",
    "DATASET_VAL_SPLIT   = \"test\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "DATASET_NAME        = DATASET_PATH.split('/')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WandB settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = f\"{MODEL_NAME}-Reward-{DATASET_NAME}\"\n",
    "os.environ[\"WANDB_ENTITY\"]  = \"RADFAN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets will be filtered according to max length\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "# Model config\n",
    "# =================================================================================================\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path   = MODEL_PATH,\n",
    "    # LoRA\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    use_peft             = True,\n",
    "    lora_task_type       = TaskType.SEQ_CLS,\n",
    "    use_rslora           = False,\n",
    "    lora_r               = 8,\n",
    "    lora_alpha           = 16,\n",
    "    lora_dropout         = 0.0,\n",
    "    lora_target_modules  = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    # Head will require grad automatically\n",
    "    lora_modules_to_save = None,\n",
    "    # Quantization\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    load_in_8bit         = False,\n",
    "    load_in_4bit         = True,\n",
    "    bnb_4bit_quant_type  = \"nf4\",\n",
    "    use_bnb_nested_quant = True,\n",
    "    torch_dtype          = \"bfloat16\",\n",
    ")\n",
    "\n",
    "# Reward trainer config\n",
    "# =================================================================================================\n",
    "\n",
    "training_args = RewardConfig(\n",
    "    # Reward trainer params\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    max_length                  = None,\n",
    "    dataset_num_proc            = 16,\n",
    "    center_rewards_coefficient  = None,\n",
    "    # Common\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    run_name                    = f\"LoRA-{model_config.lora_r}r-max_length-{MAX_LENGTH}\",\n",
    "    output_dir                  = f\"{os.environ['WANDB_PROJECT']}-LoRA-{model_config.lora_r}r\",\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size  = 4,\n",
    "    num_train_epochs            = 2,\n",
    "    gradient_checkpointing      = False,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    bf16                        = True,\n",
    "\n",
    "    # Frequency of logs\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    logging_steps               = 20,\n",
    "\n",
    "    # Evaluation\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    eval_strategy               = \"steps\",\n",
    "    eval_steps                  = 100,\n",
    "\n",
    "    # Push to hub after training\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    push_to_hub                 = False,\n",
    "    hub_model_id                = f\"RLHF-And-Friends/{MODEL_NAME}-Reward-{DATASET_NAME}-max_length-{MAX_LENGTH}\"\n",
    "                                  f\"-LoRA-{model_config.lora_r}r\"\n",
    ")\n",
    "\n",
    "# Optimizer config\n",
    "# =================================================================================================\n",
    "\n",
    "optimizer_config = OptimizerConfig(\n",
    "    optimizer_type = AdamW,\n",
    "    layer_lr       = {\n",
    "        \"lora\":  1e-5, # LoRA adapters\n",
    "        \"score\": 1e-4, # Head\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# =================================================================================================\n",
    "\n",
    "# Make quantization config\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "quantization_config = get_quantization_config(model_config)\n",
    "\n",
    "# Use KV-cache or not\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "use_cache = False if training_args.gradient_checkpointing else True\n",
    "\n",
    "# Set model type\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "if model_config.torch_dtype is not None:\n",
    "    torch_dtype = getattr(torch, model_config.torch_dtype)\n",
    "\n",
    "# Create model\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_config.model_name_or_path, \n",
    "    num_labels = 1,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\",\n",
    "    use_cache = use_cache,\n",
    "    trust_remote_code = True,\n",
    "    torch_dtype = torch_dtype\n",
    ")\n",
    "if model_config.load_in_4bit or model_config.load_in_8bit:\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model,\n",
    "        training_args.gradient_checkpointing,\n",
    "    )\n",
    "\n",
    "# Wrap in LoRA\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "lora_config = get_peft_config(model_config)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Tokenizer\n",
    "# =================================================================================================\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_config.model_name_or_path, \n",
    "    use_fast=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "\n",
    "# Sync padding tokens\n",
    "# =================================================================================================\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_PATH)\n",
    "\n",
    "train_dataset = dataset[DATASET_TRAIN_SPLIT]\n",
    "eval_dataset = dataset[DATASET_VAL_SPLIT]\n",
    "\n",
    "# Apply chat tamplate and tokenize beforehand to avoid doing it inside \n",
    "# the 'RewardTrainer'\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    load_from_cache_file = False\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    load_from_cache_file = False\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    load_from_cache_file = False\n",
    ")\n",
    "\n",
    "# Filter datasets by length (keep only examples which are no longer then \n",
    "# `max_length` tokens)\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "length_filter = (\n",
    "    lambda x: len(x[\"input_ids_chosen\"]) <= MAX_LENGTH\n",
    "              and len(x[\"input_ids_rejected\"]) <= MAX_LENGTH\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.filter(\n",
    "    length_filter,\n",
    "    num_proc=training_args.dataset_num_proc,\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.filter(\n",
    "    length_filter,\n",
    "    num_proc=training_args.dataset_num_proc,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = custom_optimizer(model, optimizer_config)\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model            = model,\n",
    "    processing_class = tokenizer,\n",
    "    args             = training_args,\n",
    "    train_dataset    = train_dataset,\n",
    "    eval_dataset     = eval_dataset,\n",
    "    optimizers       = (optimizer, None)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push model to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove added pad token from model's embedding layer\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(dataset_name=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fed-ppo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
