{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from trl import HfPairwiseJudge, OpenAIPairwiseJudge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visible devices\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "VISIBLE_DEVICES = \"0\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Enumerate GPUs based on their PCI bus IDs\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{VISIBLE_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "BASE_MODEL_PATH = \"RLHF-And-Friends/Llama-3.2-3B-Instruct\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Fine-tuned model\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "FT_MODEL_PATH = \"RLHF-And-Friends/Llama-3.2-3B-Instruct-DPO-Math\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Dataset\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "DATASET_PATH = \"HuggingFaceH4/MATH-500\"\n",
    "DATASET_SPLIT = \"test\"\n",
    "PROMPT_FIELD = \"problem\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "DATASET_NAME = DATASET_PATH.split('/')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(DATASET_PATH, split=DATASET_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(\n",
    "    prompts: list[str], \n",
    "    model_path: str, \n",
    "    batch_size: int = 8,\n",
    "    max_new_tokens: int = 512\n",
    ") -> list[str]:\n",
    "\n",
    "    chats = [[{'role': \"user\", 'content': prompt}] for prompt in prompts]\n",
    "\n",
    "    text_generator = pipeline(\n",
    "        model=model_path,\n",
    "        device_map='auto',\n",
    "        batch_size=batch_size,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    for idx in tqdm(\n",
    "        range(0, len(chats), batch_size), desc=f'{model_path} inference'\n",
    "    ):\n",
    "        batch = chats[idx:idx+batch_size]\n",
    "        responses.extend(text_generator(batch))\n",
    "\n",
    "    text_reponses = [\n",
    "        response[0]['generated_text'][-1]['content'] for response in responses\n",
    "    ]\n",
    "\n",
    "    return text_reponses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(test_dataset[PROMPT_FIELD])\n",
    "\n",
    "base_completions = get_responses(prompts, BASE_MODEL_PATH)\n",
    "ft_completions = get_responses(prompts, FT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset with models' responses and load it to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        'prompt': prompts, \n",
    "        'base_completion': base_completions,\n",
    "        'ft_completions': ft_completions\n",
    "    },\n",
    "    split = \"test\"\n",
    ")\n",
    "\n",
    "responses_dataset.push_to_hub(f\"RLHF-And-Friends/{DATASET_NAME}-Completions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset with responses and prepare to judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_dataset = load_dataset(\n",
    "    f\"RLHF-And-Friends/{DATASET_NAME}-Completions\"\n",
    ")[\"train\"].select(range(50)) # change to `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(responses_dataset['prompt'])\n",
    "completions = [\n",
    "    [base_completion, ft_completion] \n",
    "    for base_completion, ft_completion in zip(\n",
    "        responses_dataset['base_completion'], responses_dataset['ft_completions']\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge with OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_judge = OpenAIPairwiseJudge(\"gpt-4o-mini\")\n",
    "\n",
    "gpt_judgements = gpt_judge.judge(prompts, completions, shuffle_order=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_winrate = sum(gpt_judgements) / len(gpt_judgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winrate: 0.58\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT-judged winrate: {gpt_winrate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge with Huggingface API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_judge = HfPairwiseJudge(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "hf_judgements = hf_judge.judge(prompts, completions, shuffle_order=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_winrate = sum(hf_judgements) / len(hf_judgements)\n",
    "print(f\"HF-judged winrate: {hf_winrate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fed-ppo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
