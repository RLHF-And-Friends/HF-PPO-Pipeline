{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from trl import HfPairwiseJudge, OpenAIPairwiseJudge\n",
    "\n",
    "# Environment variables\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visible devices\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "VISIBLE_DEVICES = \"3\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Enumerate GPUs based on their PCI bus IDs\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{VISIBLE_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "BASE_MODEL_PATH = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Fine-tuned model\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "FT_MODEL_PATH = \"RLHF-And-Friends/Mistral-7B-LoRA-MeanRef-0.8-U13-0-1\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Dataset\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "DATASET_PATH = \"trl-internal-testing/sentiment-trl-style\"\n",
    "DATASET_SPLIT = \"test\"\n",
    "PROMPT_FIELD = \"prompt\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "DATASET_NAME = DATASET_PATH.split('/')[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompt for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = '''I require a leaderboard for various large language models. I'll provide you with prompts given to these models and their corresponding outputs. Your task is to assess these responses, and select the model that produces the best output from a human perspective.\n",
    "\n",
    "## Instruction\n",
    "\n",
    "{{\n",
    "    \"instruction\": \"\"\"{prompt}\"\"\",\n",
    "}}\n",
    "\n",
    "## Model Outputs\n",
    "\n",
    "Here are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.\n",
    "\n",
    "{{\n",
    "    {{\n",
    "        \"model_identifier\": \"0\",\n",
    "        \"output\": \"\"\"{response0}\"\"\"\n",
    "    }},\n",
    "    {{\n",
    "        \"model_identifier\": \"1\",\n",
    "        \"output\": \"\"\"{response1}\"\"\"\n",
    "    }}\n",
    "}}\n",
    "\n",
    "## Task\n",
    "\n",
    "Evaluate the models on the basis of the quality and relevance of their results, and select the model that generated the best result. Reply with the identifier of the best model. Our evaluation will only take into account the first character of your answer, so make sure it contains only one of the identifiers and nothing else (no quotation marks, no spaces, no new lines, ...).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(DATASET_PATH, split=DATASET_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(\n",
    "    prompts: list[str], \n",
    "    model_path: str, \n",
    "    batch_size: int = 8,\n",
    "    max_new_tokens: int = 512\n",
    ") -> list[str]:\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype = torch.bfloat16,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    text_generator = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map='auto',\n",
    "        batch_size=batch_size,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    chats = [[{'role': \"user\", 'content': prompt}] for prompt in prompts]\n",
    "    responses = []\n",
    "    for idx in tqdm(\n",
    "        range(0, len(chats), batch_size), desc=f'{model_path} inference'\n",
    "    ):\n",
    "        batch = chats[idx:idx+batch_size]\n",
    "        responses.extend(text_generator(batch))\n",
    "\n",
    "    text_reponses = [\n",
    "        response[0]['generated_text'][-1]['content'] for response in responses\n",
    "    ]\n",
    "\n",
    "    return text_reponses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(test_dataset[PROMPT_FIELD])\n",
    "\n",
    "base_completions = get_responses(prompts, BASE_MODEL_PATH, batch_size=32)\n",
    "ft_completions = get_responses(prompts, FT_MODEL_PATH, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset with models' responses and load it to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        'prompt': prompts, \n",
    "        'base_completion': base_completions,\n",
    "        'ft_completions': ft_completions\n",
    "    },\n",
    "    split = \"test\"\n",
    ")\n",
    "\n",
    "responses_dataset.push_to_hub(f\"RLHF-And-Friends/{DATASET_NAME}-completions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset with responses and prepare to judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES_TO_CHOOSE = 100\n",
    "responses_dataset = load_dataset(\n",
    "    f\"RLHF-And-Friends/{DATASET_NAME}-completions\"\n",
    ")[\"test\"].select(range(NUM_SAMPLES_TO_CHOOSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(responses_dataset['prompt'])\n",
    "completions = [\n",
    "    [base_completion, ft_completion] \n",
    "    for base_completion, ft_completion in zip(\n",
    "        responses_dataset['base_completion'], responses_dataset['ft_completions']\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge with OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_judge = OpenAIPairwiseJudge(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "gpt_judgements = gpt_judge.judge(prompts, completions, shuffle_order=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_winrate = sum(gpt_judgements) / len(gpt_judgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GPT-judged winrate: {gpt_winrate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fed-ppo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
