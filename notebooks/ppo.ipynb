{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    PeftModelForSequenceClassification,\n",
    "    TaskType, \n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from trl import (\n",
    "    ModelConfig,\n",
    "    PPOConfig,\n",
    "    get_peft_config,\n",
    ")\n",
    "\n",
    "from fed_ppo.ppo_trainer import CustomPPOTrainer\n",
    "from fed_ppo.utils import apply_chat_template, tokenize\n",
    "from fed_ppo.prompts import (\n",
    "    STAY_WITHIN_THE_TOKEN_LIMIT, \n",
    "    STAY_WITHIN_THE_TOKEN_LIMIT_TRAININIG_AWARE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visible devices\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "VISIBLE_DEVICES = \"5\"\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Enumerate GPUs based on their PCI bus IDs\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{VISIBLE_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy model path\n",
    "# =================================================================================================\n",
    "POLICY_PATH = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# =================================================================================================\n",
    "POLICY_NAME = POLICY_PATH.split('/')[1]\n",
    "\n",
    "# Reward model path\n",
    "# =================================================================================================\n",
    "REWARD_PATH = \"RLHF-And-Friends/Llama-3.2-1B-Instruct-Reward-ultrafeedback_binarized-LoRA-8r\"\n",
    "# =================================================================================================\n",
    "REWARD_NAME = REWARD_PATH.split('/')[1]\n",
    "\n",
    "# Prompts dataset path\n",
    "# =================================================================================================\n",
    "DATASET_PATH        = \"HuggingFaceH4/ultrachat_200k\"\n",
    "DATASET_TRAIN_SPLIT = \"train_gen\"\n",
    "DATASET_VAL_SPLIT   = \"test_gen\"\n",
    "# =================================================================================================\n",
    "DATASET_NAME        = DATASET_PATH.split('/')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WandB settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = f\"{POLICY_NAME}-PPO-{DATASET_NAME}\"\n",
    "os.environ[\"WANDB_ENTITY\"] = \"RADFAN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models' configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy\n",
    "# =================================================================================================\n",
    "\n",
    "policy_model_config = ModelConfig(\n",
    "    model_name_or_path   = POLICY_PATH,\n",
    "    # LoRA\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    use_peft             = True,\n",
    "    lora_r               = 8,\n",
    "    lora_alpha           = 16,\n",
    "    lora_dropout         = 0.0,\n",
    "    lora_task_type       = TaskType.CAUSAL_LM,\n",
    "    lora_target_modules  = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    # Quantization\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    load_in_8bit         = False,\n",
    "    load_in_4bit         = False,\n",
    "    torch_dtype          = \"bfloat16\",\n",
    ")\n",
    "\n",
    "# Value model\n",
    "# =================================================================================================\n",
    "\n",
    "value_model_config = ModelConfig(\n",
    "    # LoRA\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    use_peft            = True,\n",
    "    lora_r              = 8,\n",
    "    lora_alpha          = 16,\n",
    "    lora_dropout        = 0.0,\n",
    "    lora_task_type      = TaskType.SEQ_CLS,\n",
    "    lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    # Quantization\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    load_in_8bit        = False,\n",
    "    load_in_4bit        = False,\n",
    "    torch_dtype         = \"bfloat16\",\n",
    ")\n",
    "\n",
    "# Reward model\n",
    "# =================================================================================================\n",
    "\n",
    "reward_model_config = ModelConfig(\n",
    "    model_name_or_path  = REWARD_PATH,\n",
    "    load_in_8bit        = False,\n",
    "    load_in_4bit        = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Trainer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = PPOConfig(\n",
    "    # Common\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    exp_name            = f\"LoRA-{policy_model_config.lora_r}r-512-tokens\",\n",
    "    output_dir          = f\"{os.environ['WANDB_PROJECT']}-LoRA-{policy_model_config.lora_r}\",\n",
    "    dataset_num_proc    = 16,\n",
    "    num_mini_batches    = 1,\n",
    "    learning_rate       = 1e-5,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size  = 2,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    num_train_epochs    = 1,\n",
    "    response_length     = 512,\n",
    "    stop_token          = \"eos\",\n",
    "    # Logging\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    save_steps          = 10,\n",
    "    logging_steps       = 10,\n",
    "    eval_steps          = 10,\n",
    "    \n",
    "    # Push to hub after training\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    push_to_hub         = True,\n",
    "    hub_model_id        = f\"RLHF-And-Friends/{POLICY_NAME}-PPO-{DATASET_NAME}\"\n",
    "                          f\"-LoRA-{policy_model_config.lora_r}\",\n",
    "\n",
    "    # On-policy params\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    missing_eos_penalty = 0.0,\n",
    "    local_rollout_forward_batch_size = 1,\n",
    "\n",
    "    # PPO params\n",
    "    # ---------------------------------------------------------------------------------------------\n",
    "    num_ppo_epochs      = 1,\n",
    "    whiten_rewards      = False,\n",
    "    kl_coef             = 0.05,\n",
    "    cliprange           = 0.2,\n",
    "    vf_coef             = 0.1,\n",
    "    cliprange_value     = 0.2,\n",
    "    gamma               = 1.0,\n",
    "    lam                 = 0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize models and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "# =================================================================================================\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    policy_model_config.model_name_or_path,\n",
    "    use_fast     = True,\n",
    "    padding_side = \"left\"\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "\n",
    "\n",
    "# Models\n",
    "# =================================================================================================\n",
    "\n",
    "# SFT model\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "sft_policy = AutoModelForCausalLM.from_pretrained(\n",
    "    policy_model_config.model_name_or_path,\n",
    ")\n",
    "sft_policy.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "sft_policy.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Trainable policy\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "policy = get_peft_model(sft_policy, get_peft_config(policy_model_config))\n",
    "\n",
    "# Base model for Value and Reward models\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "base_value_head_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    policy_model_config.model_name_or_path,\n",
    "    num_labels = 1,\n",
    ")\n",
    "base_value_head_model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "base_value_head_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Value model with LoRA\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "value_model = get_peft_model(\n",
    "    base_value_head_model,\n",
    "    get_peft_config(value_model_config)\n",
    ")\n",
    "\n",
    "# Reward model\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "reward_model = PeftModelForSequenceClassification.from_pretrained(\n",
    "    base_value_head_model,\n",
    "    reward_model_config.model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d950cbab91141c4847b65ba67701546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fbc82272b94327990c67bae92cbd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a2ba178fed4e299d2dc21e3c14b2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f69288939c4b94bc475e9d7031a83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\n",
    "    DATASET_PATH, \n",
    "    split=DATASET_TRAIN_SPLIT\n",
    ").select(range(2000))\n",
    "eval_dataset = load_dataset(\n",
    "    DATASET_PATH, \n",
    "    split=DATASET_VAL_SPLIT\n",
    ").select(range(100))\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(\"messages\")\n",
    "eval_dataset = eval_dataset.remove_columns(\"messages\")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    apply_chat_template, \n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer, \n",
    "        \"system_prompt\": STAY_WITHIN_THE_TOKEN_LIMIT(512)\n",
    "    }, \n",
    "    load_from_cache_file=False\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    apply_chat_template, \n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "    },\n",
    "    load_from_cache_file = False\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize, \n",
    "    fn_kwargs={\"tokenizer\": tokenizer}, \n",
    "    load_from_cache_file = False\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    tokenize, \n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    load_from_cache_file = False\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"prompt\", \"prompt_id\"])\n",
    "eval_dataset = eval_dataset.remove_columns([\"prompt\", \"prompt_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomPPOTrainer(\n",
    "    args            = ppo_config,\n",
    "    processing_class  = tokenizer,\n",
    "    model             = policy,\n",
    "    ref_model         = sft_policy,\n",
    "    reward_model      = reward_model,\n",
    "    value_model       = value_model,\n",
    "    train_dataset     = train_dataset,\n",
    "    eval_dataset      = eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove added pad token from model's embedding layer\n",
    "\n",
    "policy.resize_token_embeddings(len(tokenizer) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(dataset_name=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fed-ppo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
