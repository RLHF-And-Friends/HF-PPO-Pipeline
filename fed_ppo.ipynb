{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import wandb\n",
    "import huggingface_hub as hub\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from accelerate import PartialState\n",
    "\n",
    "from trl import ModelConfig, PPOConfig, ScriptArguments\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n",
    "\n",
    "from utils import PolicyCommutator, CustomPPOTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]  = \"2\"\n",
    "os.environ[\"WANDB_PROJECT\"]         = \"Pythia-FedPPO\"\n",
    "os.environ[\"WANDB_ENTITY\"]          = \"RADFAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "hub.login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "NUM_AGENTS = 2\n",
    "COMMUTANT  = [\n",
    "    [0.8, 0.2],\n",
    "    [0.2, 0.8]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Configs\n",
    "###############################################################################\n",
    "\n",
    "script_args = ScriptArguments(\n",
    "    dataset_name        = \"trl-internal-testing/descriptiveness-sentiment-trl-style\",\n",
    "    dataset_train_split = \"descriptiveness\",\n",
    ")\n",
    "\n",
    "# Model to use for policies\n",
    "# =============================================================================\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path  = \"EleutherAI/pythia-70m-deduped\",\n",
    "    trust_remote_code   = False,\n",
    ")\n",
    "\n",
    "# PPO trainer config\n",
    "# =============================================================================\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    dataset_num_proc    = 1,\n",
    "    num_ppo_epochs      = 1,\n",
    "    num_train_epochs    = 0.05,\n",
    "    num_mini_batches    = 1,\n",
    "    learning_rate       = 3e-6,\n",
    "    missing_eos_penalty = 1.0,\n",
    "    per_device_train_batch_size       = 1,\n",
    "    gradient_accumulation_steps       = 16,\n",
    "    local_rollout_forward_batch_size  = 1,\n",
    "    reward_model_path   = \"EleutherAI/pythia-70m-deduped\",\n",
    "    exp_name            = \"Pythia-70M\",\n",
    "    output_dir          = \"Pythia-70M\",\n",
    "    hub_model_id        = \"RLHF-And-Friends/FedPPO-Pythia-70M\",\n",
    "    push_to_hub         = True,\n",
    ")\n",
    "\n",
    "# Distinct PPO configs\n",
    "# =============================================================================\n",
    "\n",
    "ppo_configs = [copy.copy(ppo_config) for _ in range(NUM_AGENTS)]\n",
    "\n",
    "for agent_idx, config in enumerate(ppo_configs):\n",
    "    config.exp_name     = f\"{config.exp_name}-a{agent_idx}\"\n",
    "    config.output_dir   = f\"{config.output_dir}-a{agent_idx}\"\n",
    "    config.hub_model_id = f\"{config.hub_model_id}-a{agent_idx}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  Tokenizer\n",
    "###############################################################################\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_config.model_name_or_path,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=model_config.trust_remote_code,\n",
    ")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = SIMPLE_CHAT_TEMPLATE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  Models\n",
    "###############################################################################\n",
    "\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_config.model_name_or_path\n",
    ")\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ppo_config.reward_model_path,\n",
    "    num_labels=1\n",
    ")\n",
    "\n",
    "policy_models = [\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        model_config.model_name_or_path\n",
    "    )\n",
    "    for _ in range(NUM_AGENTS)\n",
    "]\n",
    "value_models = [\n",
    "    AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_config.model_name_or_path, \n",
    "        num_labels=1\n",
    "    )\n",
    "    for _ in range(NUM_AGENTS)\n",
    "]\n",
    "\n",
    "reference_models = PolicyCommutator(\n",
    "    policies = policy_models,\n",
    "    commutant = COMMUTANT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  Dataset\n",
    "###############################################################################\n",
    "\n",
    "dataset = load_dataset(\n",
    "    script_args.dataset_name,\n",
    "    split=script_args.dataset_train_split\n",
    ")\n",
    "eval_samples = 100\n",
    "train_dataset = dataset.select(range(len(dataset) - eval_samples))\n",
    "eval_dataset = dataset.select(range(len(dataset) - eval_samples, len(dataset)))\n",
    "dataset_text_field = \"prompt\"\n",
    "\n",
    "def prepare_dataset(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    pre-tokenize the dataset before training; only collate during training\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(element):\n",
    "        outputs = tokenizer(\n",
    "            element[dataset_text_field],\n",
    "            padding=False,\n",
    "        )\n",
    "        return {\"input_ids\": outputs[\"input_ids\"]}\n",
    "\n",
    "    return dataset.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "# Compute that only on the main process for faster data processing.\n",
    "# see: https://github.com/huggingface/trl/pull/1255\n",
    "with PartialState().local_main_process_first():\n",
    "    train_dataset = prepare_dataset(train_dataset, tokenizer)\n",
    "    eval_dataset = prepare_dataset(eval_dataset, tokenizer)\n",
    "    \n",
    "train_datasets = [\n",
    "    train_dataset.shard(num_shards = NUM_AGENTS, index = agent_idx)\n",
    "    for agent_idx in range(NUM_AGENTS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  Training\n",
    "###############################################################################\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for idx in range(NUM_AGENTS):\n",
    "        trainer = CustomPPOTrainer(\n",
    "            config            = ppo_configs[idx],\n",
    "            processing_class  = tokenizer,\n",
    "            policy            = policy_models[idx],\n",
    "            ref_policy        = reference_models[idx],\n",
    "            reward_model      = reward_model,\n",
    "            value_model       = value_models[idx],\n",
    "            train_dataset     = train_datasets[idx],\n",
    "            eval_dataset      = eval_dataset,\n",
    "        )\n",
    "        trainer.train()\n",
    "        wandb.finish()\n",
    "\n",
    "        if ppo_configs[idx].push_to_hub:\n",
    "            trainer.push_to_hub(dataset_name=script_args.dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedppo",
   "language": "python",
   "name": "fedppo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
