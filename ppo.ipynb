{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    PeftModelForSequenceClassification,\n",
    "    TaskType, \n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from trl import (\n",
    "    ModelConfig, \n",
    "    PPOConfig, \n",
    "    PPOTrainer, \n",
    "    ScriptArguments,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n",
    "\n",
    "from accelerate import PartialState\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configs\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "script_args = ScriptArguments(\n",
    "    dataset_name = \"trl-internal-testing/descriptiveness-sentiment-trl-style\",\n",
    "    dataset_train_split = \"descriptiveness\",\n",
    ")\n",
    "\n",
    "# Model configs\n",
    "# =============================================================================\n",
    "\n",
    "policy_config = ModelConfig(\n",
    "    model_name_or_path = \"EleutherAI/pythia-70m-deduped\",\n",
    "    use_peft = True,\n",
    "    lora_r = 16,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05,\n",
    "    lora_target_modules = None,\n",
    "    lora_task_type = TaskType.CAUSAL_LM,\n",
    "    load_in_8bit = False,\n",
    "    load_in_4bit = False,\n",
    ")\n",
    "\n",
    "vf_config = ModelConfig(\n",
    "    use_peft = True,\n",
    "    lora_r = 8,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.01,\n",
    "    lora_target_modules = None,\n",
    "    lora_task_type = TaskType.SEQ_CLS,\n",
    "    load_in_8bit = False,\n",
    "    load_in_4bit = False,\n",
    ")\n",
    "\n",
    "reward_config = ModelConfig(\n",
    "    model_name_or_path = \"RLHF-And-Friends/Pythia-70M-Reward\",\n",
    "    use_peft=True,\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=False,\n",
    ")\n",
    "\n",
    "# PPO config\n",
    "# =============================================================================\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    # Common\n",
    "    # -------------------------------------------------------------------------\n",
    "    run_name = f\"peft_ppo_test_{1}\",\n",
    "    output_dir = f\"./ppo_{policy_config.model_name_or_path}\",\n",
    "    dataset_num_proc = 8,\n",
    "    num_mini_batches = 1,\n",
    "    learning_rate = 1e-5,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 16,\n",
    "    push_to_hub = False,\n",
    "\n",
    "    # On-policy params\n",
    "    # -------------------------------------------------------------------------\n",
    "    missing_eos_penalty = 1.0,\n",
    "    local_rollout_forward_batch_size = 1,\n",
    "\n",
    "    # PPO params\n",
    "    # -------------------------------------------------------------------------\n",
    "    reward_model_path = reward_config.model_name_or_path,\n",
    "    num_ppo_epochs = 1,\n",
    "    whiten_rewards = False,\n",
    "    kl_coef = 0.05,\n",
    "    cliprange = 0.2,\n",
    "    vf_coef = 0.1,\n",
    "    cliprange_value = 0.2,\n",
    "    gamma = 1.0,\n",
    "    lam = 0.95,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Tokenizer\n",
    "# =============================================================================\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    policy_config.model_name_or_path,\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Models\n",
    "# =============================================================================\n",
    "\n",
    "# SFT model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "sft_policy = AutoModelForCausalLM.from_pretrained(\n",
    "    policy_config.model_name_or_path,\n",
    "    quantization_config = get_quantization_config(policy_config)\n",
    ")\n",
    "\n",
    "# Trainable policy\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if policy_config.use_peft:\n",
    "    policy = get_peft_model(sft_policy, get_peft_config(policy_config))\n",
    "else:\n",
    "    policy = AutoModelForCausalLM.from_pretrained(\n",
    "        policy_config.model_name_or_path\n",
    "    )\n",
    "\n",
    "# Base model for Value and Reward models\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    policy_config.model_name_or_path,\n",
    "    num_labels = 1,\n",
    "    quantization_config = get_quantization_config(vf_config)\n",
    ")\n",
    "\n",
    "# Reward model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if reward_config.use_peft:\n",
    "    reward_model = PeftModelForSequenceClassification.from_pretrained(\n",
    "        value_model,\n",
    "        reward_config.model_name_or_path,\n",
    "        num_labels = 1,\n",
    "        quantization_config = get_quantization_config(reward_config)\n",
    "    )\n",
    "else:\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        reward_config.model_name_or_path,\n",
    "        num_labels = 1,\n",
    "        quantization_config = get_quantization_config(reward_config)\n",
    "    )\n",
    "    \n",
    "# Value model with LoRA\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if vf_config.use_peft:\n",
    "    value_model = get_peft_model(value_model, get_peft_config(vf_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Dataset\n",
    "# =============================================================================\n",
    "\n",
    "dataset = load_dataset(\n",
    "    script_args.dataset_name,\n",
    "    split=script_args.dataset_train_split\n",
    ")\n",
    "\n",
    "eval_samples = 100\n",
    "train_dataset = dataset.select(range(len(dataset) - eval_samples))\n",
    "eval_dataset = dataset.select(range(len(dataset) - eval_samples, len(dataset)))\n",
    "dataset_text_field = \"prompt\"\n",
    "\n",
    "def prepare_dataset(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    pre-tokenize the dataset before training; only collate during training\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(element):\n",
    "        outputs = tokenizer(\n",
    "            element[dataset_text_field],\n",
    "            padding=False,\n",
    "        )\n",
    "        return {\"input_ids\": outputs[\"input_ids\"]}\n",
    "\n",
    "    return dataset.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "# Compute that only on the main process for faster data processing.\n",
    "# see: https://github.com/huggingface/trl/pull/1255\n",
    "with PartialState().local_main_process_first():\n",
    "    train_dataset = prepare_dataset(train_dataset, tokenizer)\n",
    "    eval_dataset = prepare_dataset(eval_dataset, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training\n",
    "# =============================================================================\n",
    "\n",
    "trainer = PPOTrainer(\n",
    "    config            = ppo_config,\n",
    "    processing_class  = tokenizer,\n",
    "    policy            = policy,\n",
    "    ref_policy        = sft_policy,\n",
    "    reward_model      = reward_model,\n",
    "    value_model       = value_model,\n",
    "    train_dataset     = train_dataset,\n",
    "    eval_dataset      = eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fed-ppo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
