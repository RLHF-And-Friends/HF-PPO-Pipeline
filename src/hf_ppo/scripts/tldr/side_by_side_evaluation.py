from datasets import load_dataset

from trl import OpenAIPairwiseJudge

from hf_ppo.utils import judge_with_tpm_limit


'''
set API key in advance:

export OPENAI_API_KEY=<API key here>
'''

# #################################################################################################
# NAMES & PATHS
# #################################################################################################

DATASET_PATH = "RLHF-And-Friends/PPO-vs-CoPPO-TLDR-Mistral-7B-SmallSFT"
SPLIT = "test"
PROMPT_COLUMN = "prompt"
NUM_SAMPLES_TO_USE = 1000


# #################################################################################################
# Evaluation
# #################################################################################################

# System prompt
# =================================================================================================

SYSTEM_PROMPT = '''You are evaluating the performance of different language models on a summarization task. Each model is given a Reddit post from the trl-lib/tldr dataset and tasked with generating a concise and accurate summary. Your job is to compare the outputs and select the model that produces the best summary from a human perspective.

## Post

{{
    "post": """{prompt}"""",
}}

## Model completions

The following are unordered completions with summaries generated by different models. Each output is associated with a unique model identifier.

{{
    {{
        "model_identifier": "0",
        "output": """{response0}"""
    }},
    {{
        "model_identifier": "1",
        "output": """{response1}"""
    }}
}}

## Task

Select the model that generated the best summary. Reply with the identifier of the best model. Our evaluation will only consider the first character of your answer, so ensure your response contains only one of the identifiers and nothing else (no quotation marks, no spaces, no new lines, ...).
'''


# Load responses
# =================================================================================================

responses_dataset = load_dataset(DATASET_PATH)[SPLIT].select(range(NUM_SAMPLES_TO_USE))

column_names = responses_dataset.column_names
completion_columns = [
    column_name for column_name in column_names if column_name != PROMPT_COLUMN
]
assert len(completion_columns) == 2

prompts = list(responses_dataset[PROMPT_COLUMN])
completions = [
    [lhs, rhs]
    for lhs, rhs in zip(
        responses_dataset[completion_columns[0]], responses_dataset[completion_columns[1]]
    )
]

# Judge
# =================================================================================================

MODEL_TO_USE = "gpt-4o-mini"

gpt_judge = OpenAIPairwiseJudge(
    model = MODEL_TO_USE,
    system_prompt=SYSTEM_PROMPT
)

gpt_judgements = judge_with_tpm_limit(
    gpt_judge, prompts, completions, shuffle_order=False, batch_size=100
)

not_failed_judgements = [judgement for judgement in gpt_judgements if judgement != -1]

gpt_winrate = sum(not_failed_judgements) / len(not_failed_judgements)

column_names = responses_dataset.column_names
model_names = [
    column_name for column_name in column_names if column_name != PROMPT_COLUMN
]

print(f"{model_names[1]} winrate over {model_names[0]}"
      f" based on {MODEL_TO_USE} opinion: {gpt_winrate}")
