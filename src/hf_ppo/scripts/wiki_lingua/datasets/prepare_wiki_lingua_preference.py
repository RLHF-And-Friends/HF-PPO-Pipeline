from datasets import load_dataset, Dataset

from hf_ppo.arbiters.vllm_arbiter import LocalVLLMArbiter


# Paths and Constants
# =================================================================================================

# Datasets
# -------------------------------------------------------------------------------------------------

DATASET_PATH = "RLHF-And-Friends/wiki_lingua_paired"
CONFIGURATION = "en"
TEXTS_COLUMN = "text"
SUMMARIES_COLUMNS = ["summary_a", "summary_b"]
SPLITS = ["train", "validation"]

TARGET_DATASET_PATH = "RLHF-And-Friends/wiki_lingua_preference"

# Arbiter
# -------------------------------------------------------------------------------------------------

JUDGE_PATH = "Qwen/Qwen3-32B"
JUDGE_PROMPT = '''You are evaluating the performance of different language models on a summarization task. Each model is given a piece of wikipedia article and tasked with generating a concise and accurate summary. Your job is to compare the outputs and select the model that produces the best summary from a human perspective.

## Text

{{
    "text": """{text}"""",
}}

## Model responses

The following are unordered responses with summaries generated by different models. Each output is associated with a unique model identifier.

{{
    {{
        "model_identifier": "0",
        "output": """{response0}"""
    }},
    {{
        "model_identifier": "1",
        "output": """{response1}"""
    }}
}}

## Task

Select the model that generated the best summary. Reply with the identifier of the best model. Our evaluation will only consider the first character of your answer, so ensure your response contains only one of the identifiers and nothing else (no quotation marks, no spaces, no new lines, ...).
'''

# Judge
# =================================================================================================

dataset = load_dataset(DATASET_PATH, name=CONFIGURATION)

arbiter = LocalVLLMArbiter(
    system_prompt=JUDGE_PROMPT,
    model=JUDGE_PATH,
    # dtype="bfloat16"
    # tensor_parallel_size = 2,
)

for split in SPLITS:
    dataset_split = dataset[split]
    texts = dataset_split[TEXTS_COLUMN]
    completions_a = dataset_split[SUMMARIES_COLUMNS[0]]
    completions_b = dataset_split[SUMMARIES_COLUMNS[1]]

    ranks = arbiter.judge(texts, zip(completions_a, completions_b))
    
    chosen = []
    rejected = []
    for rank, (completion_a, completion_b) in zip(
        ranks, zip(completions_a, completions_b)
    ):
        if rank == 0:
            chosen.append(completion_a)
            rejected.append(completion_b)
        elif rank == 1:
            chosen.append(completion_b)
            rejected.append(completion_a)
        else:
            raise(ValueError("Value of the rank is invalid! Can not choose best response."))
        
    new_dataset_split = Dataset.from_dict({
        "text": texts,
        "chosen": chosen,
        "rejected": rejected,
    })
    
    new_dataset_split.push_to_hub(
        repo_id=TARGET_DATASET_PATH,
        config_name=CONFIGURATION,
        split=split
    )
