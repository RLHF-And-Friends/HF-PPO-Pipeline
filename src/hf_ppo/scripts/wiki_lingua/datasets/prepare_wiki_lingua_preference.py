from tqdm import tqdm

from datasets import load_dataset, Dataset

from hf_ppo.arbiters.vllm_arbiter import LocalVLLMArbiter

from vllm import SamplingParams


# Paths and Constants
# =================================================================================================

# Datasets
# -------------------------------------------------------------------------------------------------

DATASET_PATH = "RLHF-And-Friends/wiki_lingua_paired"
CONFIGURATIONS = ["en", "ru", "de", "fr", "es", "it", "nl"]
TEXTS_COLUMN = "text"
SUMMARIES_COLUMNS = ["summary_a", "summary_b"]
SPLITS = ["train", "validation"]

TARGET_DATASET_PATH = "RLHF-And-Friends/wiki_lingua_preference"

# Arbiter
# -------------------------------------------------------------------------------------------------

JUDGE_PATH = "Qwen/Qwen3-32B"
# Thinking params
# SAMPLING_PARAMS = SamplingParams(
#     max_tokens=32768,
#     temperature=0.6,
#     top_p=0.95,
#     top_k=20,
# )
# No thinking params
SAMPLING_PARAMS = SamplingParams(
    max_tokens=8192,
    temperature=0.7,
    top_p=0.8,
    top_k=20,
)
MAX_MODEL_LEN = 8192

JUDGE_PROMPT = '''/no_think You are evaluating the performance of different language models on a summarization task. Each model is given a piece of wikipedia article and tasked with generating a concise and accurate summary. Your job is to compare the outputs and select the model that produces the best summary from a human perspective.

## Text

{{
    "text": """{text}"""",
}}

## Model responses

The following are unordered responses with summaries generated by different models. Each output is associated with a unique model identifier.

{{
    {{
        "model_identifier": "0",
        "output": """{response0}"""
    }},
    {{
        "model_identifier": "1",
        "output": """{response1}"""
    }}
}}

## Task

Select the model that generated the best summary. Respond with the identifier of the best model. Our evaluation will only consider the last character of your answer, so ensure your response contains only one of the identifiers and nothing else (no quotation marks, no spaces, no new lines, ...).
'''

# Judge
# =================================================================================================

arbiter = LocalVLLMArbiter(
    system_prompt=JUDGE_PROMPT,
    model=JUDGE_PATH,
    sampling_params=SAMPLING_PARAMS,
    max_model_len=MAX_MODEL_LEN,
)

for configuration in tqdm(CONFIGURATIONS, desc="Processing configurations"):

    dataset = load_dataset(DATASET_PATH, name=configuration)

    for split in SPLITS:
        dataset_split = dataset[split]
        texts = dataset_split[TEXTS_COLUMN]
        completions_a = dataset_split[SUMMARIES_COLUMNS[0]]
        completions_b = dataset_split[SUMMARIES_COLUMNS[1]]

        ranks = arbiter.judge(texts, zip(completions_a, completions_b))
        
        chosen = []
        rejected = []
        for rank, (completion_a, completion_b) in zip(
            ranks, zip(completions_a, completions_b)
        ):
            if rank == 0:
                chosen.append(completion_a)
                rejected.append(completion_b)
            elif rank == 1:
                chosen.append(completion_b)
                rejected.append(completion_a)
            else:
                raise(
                    ValueError(
                        "Value of the rank is invalid! Arbiter could not choose best response."
                    )
                )

        print(f"Fraction of the samples where the secong summary was prefered: {sum(ranks) / len(ranks)}")
            
        new_dataset_split = Dataset.from_dict({
            "text": texts,
            "completion_a": completions_a,
            "completion_b": completions_b,
            "chosen": chosen,
            "rejected": rejected,
            "rank": ranks,
        })
        
        new_dataset_split.push_to_hub(
            repo_id=TARGET_DATASET_PATH,
            config_name=configuration,
            split=split
        )
